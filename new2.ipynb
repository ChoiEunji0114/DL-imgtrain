{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ReLU, BatchNormalization, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from six.moves import cPickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hyper parameter\n",
    "batch_size = 32\n",
    "num_classes = 100\n",
    "epochs = 30\n",
    "\n",
    "# 데이터 총 개수 : 40000개\n",
    "# 클래스 개수 : 100개, 클래스 별 400개 이미지\n",
    "\n",
    "steps_per_epoch = int(40000/batch_size)\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'trained_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as sk\n",
    "\n",
    "def load_data():\n",
    "    path = './data'\n",
    "\n",
    "    fpath = os.path.join(path, 'train_data')\n",
    "    \n",
    "    with open(fpath, 'rb') as f:\n",
    "        d = cPickle.load(f, encoding='bytes')\n",
    "        \n",
    "    X_train = d['data']\n",
    "    y_train = d['labels']\n",
    "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "x_train, y_train = load_data()\n",
    "\n",
    "# x_train : total img dataset\n",
    "# y_train : total label datset\n",
    "\n",
    "\n",
    "# test, train dataset 분류하기 \n",
    "X_train, X_test, y_train, y_test = sk.train_test_split(x_train,\n",
    "                                                    y_train,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean before normalization: 121.98364992663635\n",
      "std before normalization: 68.30227829343998\n",
      "mean after normalization: 5.568237965794131e-17\n",
      "std after normalization: 0.9999999999999987\n",
      "2.025461104398623\n"
     ]
    }
   ],
   "source": [
    "# normalize image data\n",
    "\n",
    "print (\"mean before normalization:\", np.mean(X_train)) \n",
    "print (\"std before normalization:\", np.std(X_train))\n",
    "\n",
    "mean=[0,0,0]\n",
    "std=[0,0,0]\n",
    "\n",
    "newX_train = np.ones(X_train.shape)\n",
    "newX_test = np.ones(X_test.shape)\n",
    "\n",
    "for i in range(3):\n",
    "    mean[i] = np.mean(X_train[:,:,:,i])\n",
    "    std[i] = np.std(X_train[:,:,:,i])\n",
    "       \n",
    "for i in range(3):\n",
    "    newX_train[:,:,:,i] = X_train[:,:,:,i] - mean[i]\n",
    "    newX_train[:,:,:,i] = newX_train[:,:,:,i] / std[i]\n",
    "    newX_test[:,:,:,i] = X_test[:,:,:,i] - mean[i]\n",
    "    newX_test[:,:,:,i] = newX_test[:,:,:,i] / std[i]\n",
    "\n",
    "X_train = newX_train\n",
    "X_test = newX_test\n",
    "\n",
    "print (\"mean after normalization:\", np.mean(X_train))\n",
    "print (\"std after normalization:\", np.std(X_train))\n",
    "print(X_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label preprocessing: one hot encoding\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "val_dataset = val_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "838/838 [==============================] - 73s 87ms/step - loss: 4.7776 - accuracy: 0.0733 - val_loss: 3.5897 - val_accuracy: 0.1436\n",
      "Epoch 2/30\n",
      "838/838 [==============================] - 72s 86ms/step - loss: 3.4288 - accuracy: 0.1769 - val_loss: 3.1066 - val_accuracy: 0.2323\n",
      "Epoch 3/30\n",
      "838/838 [==============================] - 74s 89ms/step - loss: 2.9113 - accuracy: 0.2734 - val_loss: 2.7291 - val_accuracy: 0.3092\n",
      "Epoch 4/30\n",
      "838/838 [==============================] - 79s 94ms/step - loss: 2.5167 - accuracy: 0.3517 - val_loss: 2.5472 - val_accuracy: 0.3540\n",
      "Epoch 5/30\n",
      "838/838 [==============================] - 77s 92ms/step - loss: 2.1739 - accuracy: 0.4278 - val_loss: 2.5278 - val_accuracy: 0.3672\n",
      "Epoch 6/30\n",
      "838/838 [==============================] - 78s 93ms/step - loss: 1.8481 - accuracy: 0.4990 - val_loss: 2.5041 - val_accuracy: 0.3807\n",
      "Epoch 7/30\n",
      "838/838 [==============================] - 77s 92ms/step - loss: 1.5219 - accuracy: 0.5746 - val_loss: 2.6441 - val_accuracy: 0.3835\n",
      "Epoch 8/30\n",
      "838/838 [==============================] - 79s 94ms/step - loss: 1.1922 - accuracy: 0.6558 - val_loss: 2.8564 - val_accuracy: 0.3681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a453d1690>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model5\n",
    "# add layer\n",
    "\n",
    "from keras.optimizers import adam\n",
    "\n",
    "model5 = Sequential()\n",
    "\n",
    "model5.add(Conv2D(50, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model5.add(Conv2D(100, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model5.add(Conv2D(200, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "model5.add(Conv2D(400, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model5.add(Flatten())\n",
    "\n",
    "model5.add(Dense(800))\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "\n",
    "model5.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss')\n",
    "]\n",
    "\n",
    "model5.fit(dataset, \n",
    "           epochs=epochs,\n",
    "           shuffle=True,\n",
    "           validation_data=val_dataset,\n",
    "           callbacks=callbacks,\n",
    "           workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "838/838 [==============================] - 80s 96ms/step - loss: 4.7668 - accuracy: 0.0703 - val_loss: 3.6346 - val_accuracy: 0.1378\n",
      "Epoch 2/30\n",
      "838/838 [==============================] - 83s 99ms/step - loss: 3.4288 - accuracy: 0.1787 - val_loss: 3.0767 - val_accuracy: 0.2330\n",
      "Epoch 3/30\n",
      "838/838 [==============================] - 78s 93ms/step - loss: 2.8877 - accuracy: 0.2785 - val_loss: 2.6744 - val_accuracy: 0.3133\n",
      "Epoch 4/30\n",
      "838/838 [==============================] - 81s 96ms/step - loss: 2.4930 - accuracy: 0.3579 - val_loss: 2.5854 - val_accuracy: 0.3374\n",
      "Epoch 5/30\n",
      "838/838 [==============================] - 80s 96ms/step - loss: 2.1574 - accuracy: 0.4287 - val_loss: 2.5072 - val_accuracy: 0.3706\n",
      "Epoch 6/30\n",
      "838/838 [==============================] - 81s 97ms/step - loss: 1.8268 - accuracy: 0.5043 - val_loss: 2.5034 - val_accuracy: 0.3905\n",
      "Epoch 7/30\n",
      "838/838 [==============================] - 79s 95ms/step - loss: 1.5045 - accuracy: 0.5810 - val_loss: 2.6686 - val_accuracy: 0.3817\n",
      "Epoch 8/30\n",
      "838/838 [==============================] - 79s 94ms/step - loss: 1.1788 - accuracy: 0.6602 - val_loss: 2.8677 - val_accuracy: 0.3817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10f1d1750>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "# model6\n",
    "# relu >> leakyrelu\n",
    "\n",
    "from keras.optimizers import adam\n",
    "\n",
    "model6 = Sequential()\n",
    "\n",
    "model6.add(Conv2D(50, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(LeakyReLU(alpha=0.05))\n",
    "model6.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model6.add(Conv2D(100, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(LeakyReLU(alpha=0.05))\n",
    "model6.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model6.add(Conv2D(200, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(LeakyReLU(alpha=0.05))\n",
    "model6.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "model6.add(Conv2D(400, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(LeakyReLU(alpha=0.05))\n",
    "model6.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model6.add(Flatten())\n",
    "\n",
    "model6.add(Dense(800))\n",
    "model6.add(Dropout(0.5))\n",
    "model6.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "\n",
    "model6.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss')\n",
    "]\n",
    "\n",
    "model6.fit(dataset, \n",
    "           epochs=epochs,\n",
    "           shuffle=True,\n",
    "           validation_data=val_dataset,\n",
    "           callbacks=callbacks,\n",
    "           workers=4)\n",
    "\n",
    "\n",
    "# 오 .. relu 보단 leakyrelu 가 약간 더 성능이 좋다. (학습 시간은 좀 더 걸리지만.. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "838/838 [==============================] - 81s 96ms/step - loss: 4.6828 - accuracy: 0.0696 - val_loss: 3.6109 - val_accuracy: 0.1322\n",
      "Epoch 2/30\n",
      "838/838 [==============================] - 80s 96ms/step - loss: 3.4560 - accuracy: 0.1718 - val_loss: 3.0518 - val_accuracy: 0.2308\n",
      "Epoch 3/30\n",
      "838/838 [==============================] - 76s 91ms/step - loss: 2.9543 - accuracy: 0.2657 - val_loss: 2.7722 - val_accuracy: 0.2899\n",
      "Epoch 4/30\n",
      "838/838 [==============================] - 87s 104ms/step - loss: 2.5595 - accuracy: 0.3430 - val_loss: 2.5989 - val_accuracy: 0.3291\n",
      "Epoch 5/30\n",
      "838/838 [==============================] - 86s 103ms/step - loss: 2.2213 - accuracy: 0.4173 - val_loss: 2.5527 - val_accuracy: 0.3558\n",
      "Epoch 6/30\n",
      "838/838 [==============================] - 85s 102ms/step - loss: 1.8938 - accuracy: 0.4858 - val_loss: 2.5803 - val_accuracy: 0.3673\n",
      "Epoch 7/30\n",
      "838/838 [==============================] - 88s 105ms/step - loss: 1.5743 - accuracy: 0.5599 - val_loss: 2.7526 - val_accuracy: 0.3618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a461a2410>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "# model7\n",
    "# optimize parameter : alpha 0.05 -> 0.01\n",
    "\n",
    "from keras.optimizers import adam\n",
    "\n",
    "model7 = Sequential()\n",
    "\n",
    "model7.add(Conv2D(50, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model7.add(BatchNormalization())\n",
    "model7.add(LeakyReLU(alpha=0.01))\n",
    "model7.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model7.add(Conv2D(100, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model7.add(BatchNormalization())\n",
    "model7.add(LeakyReLU(alpha=0.01))\n",
    "model7.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model7.add(Conv2D(200, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model7.add(BatchNormalization())\n",
    "model7.add(LeakyReLU(alpha=0.01))\n",
    "model7.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "model7.add(Conv2D(400, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model7.add(BatchNormalization())\n",
    "model7.add(LeakyReLU(alpha=0.01))\n",
    "model7.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model7.add(Flatten())\n",
    "\n",
    "model7.add(Dense(800))\n",
    "model7.add(Dropout(0.5))\n",
    "model7.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "\n",
    "model7.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss')\n",
    "]\n",
    "\n",
    "model7.fit(dataset, \n",
    "           epochs=epochs,\n",
    "           shuffle=True,\n",
    "           validation_data=val_dataset,\n",
    "           callbacks=callbacks,\n",
    "           workers=4)\n",
    "\n",
    "\n",
    "# alpha 값은 0.01 이 좋은 것 같다. 정확도가 많이 떨어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "838/838 [==============================] - 81s 97ms/step - loss: 4.6602 - accuracy: 0.0701 - val_loss: 3.6365 - val_accuracy: 0.1404\n",
      "Epoch 2/30\n",
      "838/838 [==============================] - 84s 100ms/step - loss: 3.4743 - accuracy: 0.1685 - val_loss: 3.1461 - val_accuracy: 0.2189\n",
      "Epoch 3/30\n",
      "838/838 [==============================] - 89s 107ms/step - loss: 2.9971 - accuracy: 0.2565 - val_loss: 2.7865 - val_accuracy: 0.2932\n",
      "Epoch 4/30\n",
      "838/838 [==============================] - 91s 109ms/step - loss: 2.6507 - accuracy: 0.3249 - val_loss: 2.5547 - val_accuracy: 0.3340\n",
      "Epoch 5/30\n",
      "838/838 [==============================] - 90s 108ms/step - loss: 2.3786 - accuracy: 0.3826 - val_loss: 2.4948 - val_accuracy: 0.3487\n",
      "Epoch 6/30\n",
      "838/838 [==============================] - 93s 111ms/step - loss: 2.1355 - accuracy: 0.4352 - val_loss: 2.3566 - val_accuracy: 0.3861\n",
      "Epoch 7/30\n",
      "838/838 [==============================] - 94s 112ms/step - loss: 1.9057 - accuracy: 0.4868 - val_loss: 2.3612 - val_accuracy: 0.3970\n",
      "Epoch 8/30\n",
      "838/838 [==============================] - 86s 103ms/step - loss: 1.6740 - accuracy: 0.5402 - val_loss: 2.3963 - val_accuracy: 0.3984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4b198a90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "# model8\n",
    "# add dropout\n",
    "\n",
    "from keras.optimizers import adam\n",
    "\n",
    "model8 = Sequential()\n",
    "\n",
    "model8.add(Conv2D(50, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model8.add(BatchNormalization())\n",
    "model8.add(LeakyReLU(alpha=0.05))\n",
    "model8.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model8.add(Conv2D(100, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model8.add(BatchNormalization())\n",
    "model8.add(LeakyReLU(alpha=0.05))\n",
    "model8.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model8.add(Dropout(0.1))\n",
    "\n",
    "model8.add(Conv2D(200, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model8.add(BatchNormalization())\n",
    "model8.add(LeakyReLU(alpha=0.05))\n",
    "model8.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "model8.add(Conv2D(400, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model8.add(BatchNormalization())\n",
    "model8.add(LeakyReLU(alpha=0.05))\n",
    "model8.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model8.add(Dropout(0.1))\n",
    "\n",
    "model8.add(Flatten())\n",
    "\n",
    "model8.add(Dense(800))\n",
    "model8.add(Dropout(0.5))\n",
    "model8.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "\n",
    "model8.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss')\n",
    "]\n",
    "\n",
    "model8.fit(dataset, \n",
    "           epochs=epochs,\n",
    "           shuffle=True,\n",
    "           validation_data=val_dataset,\n",
    "           callbacks=callbacks,\n",
    "           workers=4)\n",
    "\n",
    "\n",
    "# 미약하게 정확도는 올라갔으나 test accuracy 도 매우 적게 올라가기 시작했다 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "838/838 [==============================] - 82s 97ms/step - loss: 7.6274 - accuracy: 0.0580 - val_loss: 4.1734 - val_accuracy: 0.1002\n",
      "Epoch 2/30\n",
      "838/838 [==============================] - 82s 97ms/step - loss: 3.9688 - accuracy: 0.1298 - val_loss: 3.7862 - val_accuracy: 0.1586\n",
      "Epoch 3/30\n",
      "838/838 [==============================] - 83s 99ms/step - loss: 3.5824 - accuracy: 0.2041 - val_loss: 3.3973 - val_accuracy: 0.2433\n",
      "Epoch 4/30\n",
      "838/838 [==============================] - 91s 108ms/step - loss: 3.2986 - accuracy: 0.2653 - val_loss: 3.2359 - val_accuracy: 0.2774\n",
      "Epoch 5/30\n",
      "838/838 [==============================] - ETA: 0s - loss: 3.0823 - accuracy: 0.3212"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-95d50590ed3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m            workers=4)\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    927\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m    930\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TraceContext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    764\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    798\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m   1837\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1913\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1915\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1916\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1917\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "# model9\n",
    "# relu >> leakyrelu\n",
    "\n",
    "from keras.optimizers import adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model9 = Sequential()\n",
    "\n",
    "model9.add(Conv2D(50, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model9.add(BatchNormalization())\n",
    "model9.add(LeakyReLU(alpha=0.05))\n",
    "model9.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model9.add(Conv2D(100, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model9.add(BatchNormalization())\n",
    "model9.add(LeakyReLU(alpha=0.05))\n",
    "model9.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model9.add(Conv2D(200, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model9.add(BatchNormalization())\n",
    "model9.add(LeakyReLU(alpha=0.05))\n",
    "model9.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "\n",
    "model9.add(Conv2D(400, (3, 3), \n",
    "                  padding='same',\n",
    "                  input_shape=X_train.shape[1:],))\n",
    "model9.add(BatchNormalization())\n",
    "model9.add(LeakyReLU(alpha=0.05))\n",
    "model9.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model9.add(Flatten())\n",
    "\n",
    "model9.add(Dense(800))\n",
    "model9.add(Dropout(0.5))\n",
    "model9.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "adam = optimizers.Adam(lr = 0.001)\n",
    "\n",
    "model9.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss')\n",
    "]\n",
    "\n",
    "model9.fit(dataset, \n",
    "           epochs=epochs,\n",
    "           shuffle=True,\n",
    "           validation_data=val_dataset,\n",
    "           callbacks=callbacks,\n",
    "           workers=4)\n",
    "\n",
    "\n",
    "# 오 .. relu 보단 leakyrelu 가 약간 더 성능이 좋다. (학습 시간은 좀 더 걸리지만.. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
